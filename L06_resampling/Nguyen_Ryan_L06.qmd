---
title: "L06 Resampling"
subtitle: "Data Science 2 with R (STAT 301-2)"
author: "Ryan Nguyen"

format:
  html:
    toc: true
    toc-depth: 4
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false
  
from: markdown+emoji 
---

## Overview

This lab covers material up to and including [11. Comparing models with resampling (11.2)](https://www.tmwr.org/compare.html) from [Tidy Modeling with R](https://www.tmwr.org/).

::: {.callout-tip}

**Successful science requires organization!**

As we increase the the number of models to train along with the number of times we will be training/fitting the models (resamples!), organization will be critical for carrying out the entire machine learning process. 

Thoughtful organization will help handle the increasing computational demands, streamline the analysis process, and aide in the communication of results.

Thoughtful organization doesn't take one form, but we will demonstrate one possible setup during class.

:::

```{r}
library(tidyverse)
library(tidymodels)
library(kableExtra)
set.seed(24)
```

## Tasks

### Task 1

Use the tidyverse function `read_csv()` to load the `kc_house_data.csv` file from the `\data` directory into R. The data set contains 21,613 house sale prices (`price`) and other information for homes sold between May 2014 and May 2015 in King County, WA.

We have used this dataset repeatedly, but it is important to *"know thy data"*. So, take a moment to read the variable definitions in `kc_house_data_codebook.txt`.

```{r}
# loading in data
kc_data <- read_csv("data/kc_house_data.csv") %>% 
  janitor::clean_names()
```


### Task 2

As before, our objective is to develop a predictive model for the house sale `price`.

From our previous work we determined that we should log-transform (base 10) `price`. Apply a log-transformation.

Typically we would also perform a quick data assurance check using `skim_without_charts()` to see if there are any major issues. We're mostly checking for missing data problems, but we also look for any obvious read-in issues. We've done this in past labs and we haven't noted any issues so we should be able to proceed.

```{r}
kc_data <- kc_data %>% 
  select(-c(id, date, zipcode)) %>% 
  mutate(price_log = log10(price),
         waterfront = factor(waterfront),
         view = factor(view, ordered = TRUE),
         condition = factor(condition, ordered = TRUE),
         grade = factor(grade, ordered = TRUE)) 
```


### Task 3

Split the data into training and test sets. You can choose what proportion to use. Use stratified sampling. Verify that the training and test sets have the correct dimensions.

```{r}
kc_split <- initial_split(kc_data, prob = 0.8, strata = price_log)
  
kc_train <- training(kc_split)
kc_test <- testing(kc_split)

# View(kc_train)
# View(kc_test)
```
kc_train has 16,209 observations, and kc_test has 5,404. 

### Task 4

Fold the training data. Use repeated V-fold cross-validation, with 5 folds and 3 repeats.
```{r}
kc_folds <- vfold_cv(kc_train, v = 5, repeats = 3,
                           strata = price_log)
```


### Task 5

In your own words, **explain what we are doing** in Task 4. What is repeated V-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set?

**ANSWER:** In V-fold cross-validation, data is randomly split into V sets of approximately equal size, with V sets of performance metrics. In the end, the final performance measure is the average of each replicates.  This is better than simply fitting and testing models because it helps prevent overfitting and improves accuracy of population parameters, by allowing us to take uncertainty into account.

### Task 6

Looking ahead, we plan on fitting 4 model types: **standard linear, random forest, ridge, and lasso**. All of our variables for prediction are numerical. The categorical variables are either binary (so we can leave them as they are) or those with multiple categories have been turned into indices for us. This means that we will be able to use the same recipe for all 4 models we plan to fit. **Create the following recipe:**

1.  Predict the target variable with all other variables
2.  Do not use `id`, `date`, or `zipcode` as predictors (might have to exclude `price` too, depends on how log-transformation was handled)
3.  Log-transform `sqft_living, sqft_lot, sqft_above,  sqft_living15, sqft_lot15`
4. Turn `sqft_basement` into an indicator variable (if greater than 0 house has basement, otherwise it does not have basement),
4.  Center all predictors
5.  Scale all predictors.

```{r}
kc_recipe <- recipe(price_log ~., data = kc_train) %>% 
  step_log(sqft_living, sqft_lot, sqft_above, sqft_living15, sqft_lot15) %>%
  step_mutate(sqft_basement = factor(ifelse(sqft_basement == 0, "no", "yes"))) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```


### Task 7

Set up workflows for 4 models:

1.  A linear regression (`linear_reg()`) with the `"lm"` engine,
2.  A ridge regression (`linear_reg(penalty = 0.001, mixture = 0)`), with the `"glmnet"` engine,
3.  A lasso regression (`linear_reg(penalty = 0.001, mixture = 1)`), with the `"glmnet"` engine, and
4.  A random forest (`rand_forest()`) with the `"ranger"` engine setting `min_n = 10` and `trees = 600`. We will use the default value for `mtry`.

See the challenge below for a fifth model to fit --- **graduate students are required to fit this model**.

```{r}
## Linear Regression
lm_spec <- linear_reg() %>% 
  set_engine("lm")

lm_workflow <- workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(kc_recipe)

## Lasso
lasso_spec <- linear_reg(penalty = 0.01, mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

lasso_workflow <- workflow() %>% 
  add_model(lasso_spec) %>% 
  add_recipe(kc_recipe)

## Ridge
ridge_spec <- linear_reg(penalty = 0.01, mixture = 0) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

ridge_workflow <- workflow() %>% 
  add_model(ridge_spec) %>% 
  add_recipe(kc_recipe)

## Random forest spec
rf_spec <- rand_forest(trees = 600, min_n = 10) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(kc_recipe)
```


### Task 8

Fit each of the four models created in Task 7 to the folded data.

::: {.callout-important}

Some models, especially random forests, may take a while to run -- anywhere from 3 to 10 minutes. You should **NOT** re-run these models each time you render. Instead, run them once, using an R script, and store your results. You should still include the code to run them when you render, but set `eval: false`. You may also need to use hidden R chunks (reading in saved files) to present your work.

:::

```{r}
#| eval: FALSE

# fit lm folds
lm_fit_folds <- fit_resamples(lm_workflow,
                              resamples = kc_folds,
                              control = keep_pred)

write_rds(lm_fit_folds, file = "results/lm_fit_folds.rds")

# fit lasso folds
lasso_fit_folds <- fit_resamples(lasso_workflow,
                                 resamples = kc_folds,
                                 control = keep_pred)

write_rds(lasso_fit_folds, file = "results/lasso_fit_folds.rds")

# fit ridge folds
ridge_fit_folds <- fit_resamples(ridge_workflow,
                                 resamples = kc_folds,
                                 control = keep_pred)

write_rds(ridge_fit_folds, file = "results/ridge_fit_folds.rds")

# rf fit folds
rf_fit_folds <- fit_resamples (rf_workflow,
                               resamples = kc_folds,
                               control = keep_pred)

write_rds(rf_fit_folds, file = "results/rf_fit_folds.rds")
```


### Task 9

Use `collect_metrics()` to print the mean and standard errors of the performance metrics RMSE and *R*^2^ across all folds for each of the four models.

Decide which of the 4 fitted models has performed the best. Explain why. *Note: You should consider both the mean RMSE and its standard error.*

```{r}
# load folds in 
lm_fit_folds <- read_rds("results/lm_fit_folds.rds")
lasso_fit_folds <- read_rds("results/lasso_fit_folds.rds")
ridge_fit_folds <- read_rds("results/ridge_fit_folds.rds")
rf_fit_folds <- read_rds("results/rf_fit_folds.rds")

# print mean and std_error of perf metrics for all 4 models
results <- lm_fit_folds %>% 
  collect_metrics() %>% 
  mutate(model = "lm") %>% 
  bind_rows(lasso_fit_folds %>% 
              collect_metrics() %>% 
              mutate(model = "lasso")) %>% 
  bind_rows(ridge_fit_folds %>% 
              collect_metrics() %>% 
              mutate(model = "ridge")) %>% 
  bind_rows(rf_fit_folds %>% 
              collect_metrics() %>% 
              mutate(model = "rf"))

results %>% 
  kbl() %>% 
  kable_styling()
```

**ANSWER:** The random forest model has performed the best since it has the lowest RMSE and highest RSQ. Its RMSE has the second lowest standard error out of all RMSE. Its RSQ has lowest standard error out of all RSQ.

### Task 10

Now that you've chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
final_fit <- fit(rf_workflow, kc_train)
```

### Task 11

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `rmse()` to assess your model's performance on the **testing** data! You may also assess it using *R*^2^.

Compare your model's testing RMSE to its average RMSE across folds.

```{r}
kc_results <- kc_test %>% 
  select(price_log) %>% 
  bind_cols(predict(final_fit, kc_test)) %>% 
  rmse(price_log, .pred)

kc_results

rf_results <- rf_fit_folds %>% 
  collect_metrics() %>% 
  mutate(model = "rf")

rf_results

```

**ANSWER:** Our random forest model's RMSE is 0.0317, while its average RMSE across folds is 0.0792. Therefore, the testing model's RMSE is less than the its average RMSE across folds. 

## Challenge

**Required** for graduate students, but not for undergraduates (recommended though).

### Additional model

In addition to the 4 models we are using above, take the necessary steps to include a nearest neighbors model using between 15 and 50 `neighbors` --- your choice. Leave other tuning parameters set at their defaults. Suggest looking over the documentation for `nearest_neighbors()`.

In a sentence or two, provide a quick overview of how a nearest neighbor model works.

Is it a mechanistic or empirically driven model?
